"""
Constrained IGS Predictor

This model produces realistic, gradual IGS predictions by:
1. Using Bayesian Ridge Regression for better uncertainty with small data
2. Applying domain-knowledge constraints on yearly changes
3. Incorporating trend momentum from historical data
4. Calibrating intervention effects based on empirical observations

Key insight: Real-world IGS improvements are gradual (typically 0.3-1.5 pts/year)
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import BayesianRidge, Ridge
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import TimeSeriesSplit
import joblib
import os
from typing import List, Dict, Tuple, Optional
import warnings

warnings.filterwarnings('ignore')

# Paths
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
IGS_PATH = os.path.join(BASE_DIR, 'data/igs_talladega_tracts.csv')
ACS_PATH = os.path.join(BASE_DIR, 'data/acs_talladega_tracts.csv')
MODEL_PATH = os.path.join(os.path.dirname(__file__), 'igs_constrained_model.joblib')


# ============================================================================
# DOMAIN KNOWLEDGE CONSTRAINTS
# ============================================================================

# Realistic yearly change bounds based on IGS methodology
# Most tracts show -2 to +3 points per year change
YEARLY_CHANGE_BOUNDS = {
    'min': -2.0,   # Worst case yearly decline
    'max': 3.5,    # Best case yearly improvement
    'typical_low': 0.3,
    'typical_high': 1.2
}

# Intervention impact multipliers (calibrated to be realistic)
# These multiply the base yearly change when interventions are active
INTERVENTION_MULTIPLIERS = {
    'digital': {
        'base_boost': 0.4,      # +0.4 pts/year from digital infrastructure
        'max_boost': 0.8,       # Ceiling on digital impact
        'ramp_years': 2         # Years to reach full effect
    },
    'housing': {
        'base_boost': 0.35,
        'max_boost': 0.7,
        'ramp_years': 3         # Housing takes longer to show effect
    },
    'entrepreneurship': {
        'base_boost': 0.25,
        'max_boost': 0.5,
        'ramp_years': 2
    },
    'workforce': {
        'base_boost': 0.3,
        'max_boost': 0.6,
        'ramp_years': 1.5       # Faster impact from workforce programs
    },
    'health': {
        'base_boost': 0.2,
        'max_boost': 0.4,
        'ramp_years': 2
    },
    'policy': {
        'base_boost': 0.15,
        'max_boost': 0.35,
        'ramp_years': 3         # Policy changes are slow
    }
}

# Combined intervention synergy bonus (when multiple interventions active)
SYNERGY_BONUS = 0.08  # Per additional intervention beyond the first


class ConstrainedIGSPredictor:
    """
    A constrained Bayesian Ridge predictor for IGS scores.
    
    Key Features:
    - Uses Bayesian Ridge for better small-sample performance
    - Applies realistic bounds on yearly changes
    - Models intervention effects with ramp-up periods
    - Incorporates trend momentum from historical data
    """
    
    def __init__(self, model_path: str = MODEL_PATH):
        self.model_path = model_path
        self.model = None
        self.scaler = None
        self.igs_df = None
        self.acs_df = None
        self.tract_trends = {}  # Store historical trends per tract
        self._load_data()
        
    def _load_data(self):
        """Load IGS and ACS data"""
        try:
            self.igs_df = pd.read_csv(IGS_PATH, dtype={'Census Tract FIPS code': str})
            self.acs_df = pd.read_csv(ACS_PATH, dtype={'tract_fips': str})
            
            # Normalize FIPS codes
            self.igs_df['Census Tract FIPS code'] = (
                self.igs_df['Census Tract FIPS code'].astype(str).str.zfill(10)
            )
            self.acs_df['tract_fips'] = (
                self.acs_df['tract_fips'].astype(str).str.zfill(11)
            )
            
            # Precompute tract trends
            self._compute_tract_trends()
            
        except FileNotFoundError as e:
            print(f"Warning: Data file not found: {e}")
        except Exception as e:
            print(f"Warning: Error loading data: {e}")
    
    def _compute_tract_trends(self):
        """Compute historical trends for each tract"""
        if self.igs_df is None:
            return
            
        for tract in self.igs_df['Census Tract FIPS code'].unique():
            tract_data = self.igs_df[
                self.igs_df['Census Tract FIPS code'] == tract
            ].sort_values('Year')
            
            if len(tract_data) < 2:
                self.tract_trends[tract] = {'trend': 0, 'volatility': 1.0}
                continue
            
            igs_scores = tract_data['Inclusive Growth Score'].values
            years = tract_data['Year'].values
            
            # Calculate yearly changes
            yearly_changes = np.diff(igs_scores)
            
            # Compute trend (weighted toward recent years)
            weights = np.linspace(0.5, 1.0, len(yearly_changes))
            weighted_trend = np.average(yearly_changes, weights=weights)
            
            # Compute volatility (std of changes)
            volatility = np.std(yearly_changes) if len(yearly_changes) > 1 else 1.0
            
            # Store latest value for reference
            self.tract_trends[tract] = {
                'trend': weighted_trend,
                'volatility': volatility,
                'latest_igs': igs_scores[-1],
                'latest_year': years[-1],
                'mean_igs': np.mean(igs_scores),
                'history': list(igs_scores)
            }
    
    def _get_features(self, tract: str) -> Optional[np.ndarray]:
        """Get ACS features for a tract"""
        if self.acs_df is None:
            return None
            
        tract_acs = self.acs_df[
            self.acs_df['tract_fips'].str[-10:] == tract
        ]
        
        if tract_acs.empty:
            return None
        
        # Key features for prediction
        feature_cols = [
            'median_household_income', 'gini_index', 'per_capita_income',
            'poverty_rate', 'unemployment_rate', 'labor_force_participation_rate',
            'housing_cost_burden_rate', 'broadband_access_rate'
        ]
        
        features = []
        for col in feature_cols:
            val = tract_acs.iloc[0].get(col, 0)
            try:
                features.append(float(val) if pd.notna(val) else 0)
            except:
                features.append(0)
        
        return np.array(features)
    
    def train(self, alpha_1: float = 1e-6, alpha_2: float = 1e-6) -> 'ConstrainedIGSPredictor':
        """
        Train the Bayesian Ridge model on historical IGS data.
        
        Uses year-over-year changes as targets to learn the dynamics
        rather than absolute values.
        """
        if self.igs_df is None:
            raise ValueError("No IGS data loaded")
        
        X, y = [], []
        
        for tract in self.igs_df['Census Tract FIPS code'].unique():
            features = self._get_features(tract)
            if features is None:
                continue
            
            tract_data = self.igs_df[
                self.igs_df['Census Tract FIPS code'] == tract
            ].sort_values('Year')
            
            igs_scores = tract_data['Inclusive Growth Score'].values
            
            # Create training examples from year-over-year changes
            for i in range(len(igs_scores) - 1):
                # Features: ACS data + current IGS + trend info
                current_igs = igs_scores[i]
                trend = self.tract_trends[tract]['trend']
                
                feature_vector = np.concatenate([
                    features,
                    [current_igs, trend, i]  # Add temporal features
                ])
                
                X.append(feature_vector)
                y.append(igs_scores[i + 1] - igs_scores[i])  # Predict CHANGE
        
        if len(X) == 0:
            raise ValueError("No training data available")
        
        X = np.array(X)
        y = np.array(y)
        
        # Robust scaling (better for outliers in small data)
        self.scaler = RobustScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # Bayesian Ridge - better uncertainty estimation for small data
        self.model = BayesianRidge(
            alpha_1=alpha_1,
            alpha_2=alpha_2,
            lambda_1=1e-6,
            lambda_2=1e-6,
            compute_score=True,
            fit_intercept=True,
            max_iter=300
        )
        self.model.fit(X_scaled, y)
        
        # Evaluate with time series cross-validation
        tscv = TimeSeriesSplit(n_splits=min(3, len(X) // 4))
        cv_scores = []
        for train_idx, val_idx in tscv.split(X_scaled):
            temp_model = BayesianRidge()
            temp_model.fit(X_scaled[train_idx], y[train_idx])
            preds = temp_model.predict(X_scaled[val_idx])
            rmse = np.sqrt(np.mean((preds - y[val_idx]) ** 2))
            cv_scores.append(rmse)
        
        print(f"Model trained on {len(X)} samples")
        print(f"Cross-validation RMSE: {np.mean(cv_scores):.3f} Â± {np.std(cv_scores):.3f}")
        print(f"Mean yearly change in training: {np.mean(y):.3f}")
        
        # Save model
        self.save()
        
        return self
    
    def save(self):
        """Save model and scaler"""
        joblib.dump({
            'model': self.model,
            'scaler': self.scaler,
            'tract_trends': self.tract_trends
        }, self.model_path)
        print(f"Model saved to: {self.model_path}")
    
    def load(self) -> 'ConstrainedIGSPredictor':
        """Load trained model"""
        if not os.path.exists(self.model_path):
            print("No saved model found. Training new model...")
            return self.train()
        
        data = joblib.load(self.model_path)
        self.model = data['model']
        self.scaler = data['scaler']
        self.tract_trends = data.get('tract_trends', {})
        return self
    
    def _compute_intervention_boost(
        self, 
        interventions: List[str], 
        year: int
    ) -> float:
        """
        Compute the intervention boost for a given year.
        
        Interventions ramp up over time and have synergy effects.
        """
        if not interventions:
            return 0.0
        
        total_boost = 0.0
        
        for intervention in interventions:
            if intervention not in INTERVENTION_MULTIPLIERS:
                continue
            
            config = INTERVENTION_MULTIPLIERS[intervention]
            ramp_years = config['ramp_years']
            base_boost = config['base_boost']
            max_boost = config['max_boost']
            
            # Ramp-up factor (0 to 1 over ramp_years)
            ramp_factor = min(1.0, (year + 1) / ramp_years)
            
            # Apply diminishing returns at higher years
            diminishing = 1.0 / (1.0 + 0.1 * max(0, year - 3))
            
            boost = base_boost * ramp_factor * diminishing
            boost = min(boost, max_boost)
            
            total_boost += boost
        
        # Synergy bonus for multiple interventions
        if len(interventions) > 1:
            synergy = SYNERGY_BONUS * (len(interventions) - 1)
            total_boost += synergy
        
        return total_boost
    
    def predict(
        self,
        tract: str,
        interventions: Optional[List[str]] = None,
        years_ahead: int = 5
    ) -> List[float]:
        """
        Predict future IGS scores with realistic constraints.
        
        Args:
            tract: Census tract FIPS code
            interventions: List of intervention types
            years_ahead: Number of years to predict
            
        Returns:
            List of predicted IGS scores
        """
        # Ensure model is loaded
        if self.model is None:
            self.load()
        
        # Normalize tract code
        tract = str(tract).zfill(10)
        interventions = interventions or []
        
        # Get tract info
        trend_info = self.tract_trends.get(tract)
        if trend_info is None:
            # Fallback to conservative estimate
            trend_info = {'latest_igs': 23, 'trend': 0.5, 'volatility': 1.5}
        
        current_igs = trend_info['latest_igs']
        base_trend = trend_info['trend']
        
        # Get features for ML prediction
        features = self._get_features(tract)
        
        predictions = []
        running_igs = current_igs
        
        for year in range(years_ahead):
            # 1. Get ML-based prediction of yearly change
            if features is not None and self.model is not None:
                feature_vector = np.concatenate([
                    features,
                    [running_igs, base_trend, year]
                ])
                X_scaled = self.scaler.transform([feature_vector])
                
                # Bayesian Ridge gives us mean and std
                ml_change, ml_std = self.model.predict(X_scaled, return_std=True)
                ml_change = ml_change[0]
            else:
                # Fallback to trend-based prediction
                ml_change = base_trend
            
            # 2. Add intervention boost
            intervention_boost = self._compute_intervention_boost(interventions, year)
            
            # 3. Combine ML prediction with intervention boost
            total_change = ml_change + intervention_boost
            
            # 4. Apply domain constraints
            # Constrain to realistic yearly changes
            total_change = np.clip(
                total_change,
                YEARLY_CHANGE_BOUNDS['min'],
                YEARLY_CHANGE_BOUNDS['max']
            )
            
            # Add slight regression to mean for extreme values
            if running_igs > 70:
                total_change *= 0.7  # Harder to improve at high levels
            elif running_igs < 25:
                total_change = max(total_change, 0.3)  # Floor on very low tracts
            
            # 5. Update running IGS
            running_igs += total_change
            
            # Bound to valid IGS range
            running_igs = np.clip(running_igs, 0, 100)
            
            predictions.append(round(running_igs, 2))
        
        return predictions
    
    def predict_with_confidence(
        self,
        tract: str,
        interventions: Optional[List[str]] = None,
        years_ahead: int = 5
    ) -> Dict:
        """
        Predict with confidence intervals.
        
        Returns:
            Dict with 'predictions', 'lower_bound', 'upper_bound'
        """
        predictions = self.predict(tract, interventions, years_ahead)
        
        # Compute confidence bounds based on historical volatility
        tract = str(tract).zfill(10)
        trend_info = self.tract_trends.get(tract, {'volatility': 1.5})
        volatility = trend_info.get('volatility', 1.5)
        
        lower = []
        upper = []
        
        for i, pred in enumerate(predictions):
            # Uncertainty grows with time
            uncertainty = volatility * np.sqrt(i + 1) * 0.8
            lower.append(round(max(0, pred - uncertainty), 2))
            upper.append(round(min(100, pred + uncertainty), 2))
        
        return {
            'predictions': predictions,
            'lower_bound': lower,
            'upper_bound': upper,
            'interventions': interventions or [],
            'years': list(range(1, years_ahead + 1))
        }


def main():
    """Test the constrained predictor"""
    print("=" * 60)
    print("Testing Constrained IGS Predictor")
    print("=" * 60)
    
    predictor = ConstrainedIGSPredictor()
    
    # Train
    print("\n1. Training model...")
    predictor.train()
    
    # Test predictions
    print("\n2. Testing predictions...")
    
    test_tract = '1121010500'  # Tract 105
    
    # Baseline (no interventions)
    print(f"\n   Tract {test_tract} - Baseline (no interventions):")
    baseline = predictor.predict(test_tract, interventions=[], years_ahead=5)
    for i, pred in enumerate(baseline, 1):
        print(f"      Year +{i}: {pred:.2f}")
    
    # With interventions
    print(f"\n   Tract {test_tract} - With digital + housing interventions:")
    with_interventions = predictor.predict(
        test_tract, 
        interventions=['digital', 'housing'], 
        years_ahead=5
    )
    for i, pred in enumerate(with_interventions, 1):
        diff = pred - baseline[i-1]
        print(f"      Year +{i}: {pred:.2f} (+{diff:.2f} vs baseline)")
    
    # With all interventions
    print(f"\n   Tract {test_tract} - With ALL interventions:")
    all_interventions = predictor.predict(
        test_tract,
        interventions=['digital', 'housing', 'entrepreneurship', 'workforce'],
        years_ahead=5
    )
    for i, pred in enumerate(all_interventions, 1):
        diff = pred - baseline[i-1]
        print(f"      Year +{i}: {pred:.2f} (+{diff:.2f} vs baseline)")
    
    # With confidence
    print(f"\n3. Prediction with confidence intervals:")
    result = predictor.predict_with_confidence(
        test_tract,
        interventions=['digital', 'housing'],
        years_ahead=5
    )
    for i in range(5):
        print(f"      Year +{i+1}: {result['predictions'][i]:.2f} "
              f"[{result['lower_bound'][i]:.2f}, {result['upper_bound'][i]:.2f}]")


if __name__ == '__main__':
    main()

